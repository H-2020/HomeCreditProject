{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/cb/a8ec24334c35a7d0c87b4e4e056bd2137573c7c1bd81c760b79a2f370254/lightgbm-2.3.1-py2.py3-none-win_amd64.whl (544kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\sony\\anaconda3\\lib\\site-packages (from lightgbm) (1.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sony\\anaconda3\\lib\\site-packages (from lightgbm) (1.16.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sony\\anaconda3\\lib\\site-packages (from lightgbm) (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sony\\anaconda3\\lib\\site-packages (from scikit-learn->lightgbm) (0.13.2)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-2.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_columns',700)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                #    df[col] = df[col].astype(np.float16)\n",
    "                #elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                #    df[col] = df[col].astype(np.float32)\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:100].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances01.png')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/train_transaction.csv' does not exist: b'data/train_transaction.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-000b8e2a3c3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_transaction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/train_transaction.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_identity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/train_identity.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_transaction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/test_transaction.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_identity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/test_identity.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/train_transaction.csv' does not exist: b'data/train_transaction.csv'"
     ]
    }
   ],
   "source": [
    "train_transaction = pd.read_csv(\"data/train_transaction.csv\")\n",
    "train_identity = pd.read_csv(\"data/train_identity.csv\")\n",
    "\n",
    "test_transaction = pd.read_csv(\"data/test_transaction.csv\")\n",
    "test_identity = pd.read_csv(\"data/test_identity.csv\")\n",
    "\n",
    "# Fix column name \n",
    "fix_col_name = {testIdCol:trainIdCol for testIdCol, trainIdCol in zip(test_identity.columns, train_identity.columns)}\n",
    "test_identity.rename(columns=fix_col_name, inplace=True)\n",
    "    \n",
    "## Reduce memory\n",
    "train_transaction = reduce_mem_usage(train_transaction)\n",
    "train_identity = reduce_mem_usage(train_identity)\n",
    "\n",
    "test_transaction = reduce_mem_usage(test_transaction)\n",
    "test_identity = reduce_mem_usage(test_identity)\n",
    "    \n",
    "# Merge (transaction-identity) DATA\n",
    "train_temp = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "test_temp = test_transaction.merge(test_identity, on='TransactionID', how='left')\n",
    "\n",
    "#MERGE (X_train - X_test)\n",
    "train_test_temp = pd.concat([train_temp, test_temp], ignore_index=True)\n",
    "\n",
    "print(f'train dataset has {train_temp.shape[0]} rows and {train_temp.shape[1]} columns.')\n",
    "print(f'test dataset has {test_temp.shape[0]} rows and {test_temp.shape[1]} columns.')\n",
    "\n",
    "del train_transaction, train_identity, test_transaction, test_identity; x = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train_test_temp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col_list = []\n",
    "LE_col_list = []\n",
    "OHE_col_list = []\n",
    "# Cok sinifli kategorik degiskenler icin bunu kullanacagim. \n",
    "LGBM_cat_col_list =[]\n",
    "cat_cols = ['ProductCD','card1','card2','card3','card4','card5','card6','addr1','addr2','P_emaildomain','R_emaildomain',\n",
    "            'M1','M2','M3','M4','M5','M6','M7','M8','M9','DeviceType','DeviceInfo'] + [f'id_{i}' for i in range(12,39)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransactionDT Yi isle\n",
    "# ENCODING STRATEJISI BELIRLE : ohe uygulanabilir yeni turetilecek degiskenler icin  \n",
    "import datetime\n",
    "START_DATE = '2019-04-22'\n",
    "startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "train_test['NewDate'] = train_test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n",
    "train_test['NewDate_YMD'] = train_test['NewDate'].dt.year.astype(str) + '-' + train_test['NewDate'].dt.month.astype(str) + '-' + train_test['NewDate'].dt.day.astype(str)\n",
    "train_test['NewDate_YearMonth'] = train_test['NewDate'].dt.year.astype(str) + '-' + train_test['NewDate'].dt.month.astype(str)\n",
    "train_test['NewDate_Weekday'] = train_test['NewDate'].dt.dayofweek\n",
    "train_test['NewDate_Hour'] = train_test['NewDate'].dt.hour\n",
    "train_test['NewDate_Day'] = train_test['NewDate'].dt.day\n",
    "\n",
    "drop_col_list.append(\"TransactionDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransactionAMT\n",
    "train_test['New_Cents'] = (train_test['TransactionAmt'] - np.floor(train_test['TransactionAmt'])).astype('float32')\n",
    "train_test['New_TransactionAmt_Bin'] = pd.qcut(train_test['TransactionAmt'],10)\n",
    "\n",
    "LE_col_list.append(\"New_TransactionAmt_Bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProductCD--> 5 FARKLI deger var bos deger yok, ohe yapilacak, frequency encoding uygulanabilir\n",
    "OHE_col_list.append('ProductCD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cardX\n",
    "# ENCODING STRATEJISI : \n",
    "# Card degerleri categorik  --> frekans encoding uygulanabilir, \n",
    "#                               lighbtm ye cat diye belirtilebilir,\n",
    "#                               le uygulanabilir .\n",
    "#                               ohe uygulanabilir.\n",
    "#                               sayisalsa direk birakilabilir. \n",
    "#100 den fazla kategorik olanlari ya le yada ligbtm cat uygulayacagim. \n",
    "#card1 - card2 _addr1 _addr2 kombinasyonlarindan yeni degiskenler olusturulup yukariaki encoding islemleri denenebilir. \n",
    "#card1 - 13553 \n",
    "#card2 - 500\n",
    "#card3 - 114\n",
    "#card4 - 4 \n",
    "#card5 - 119\n",
    "#card6 - 4\n",
    "# NAN DEGER STRATEJISI :  !!! BELIRLENECEK.\n",
    "\n",
    "\n",
    "LGBM_cat_col_list.append('card1')\n",
    "LGBM_cat_col_list.append('card2')\n",
    "LGBM_cat_col_list.append('card3')\n",
    "LGBM_cat_col_list.append('card5')\n",
    "OHE_col_list.append('card4')\n",
    "OHE_col_list.append('card6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addr1 & addr2 --> KATEGORIK DEGISKEN BIRINDE  addr1 de 332 adr2 de 74 farkli sinif var\n",
    "# ENCODING STRATEJISI : le veya oldugu gibi birakmakta denenebilir.  \n",
    "# NAN DEGER STRATEJISI :  !!! BELIRLENECEK. \n",
    "# FE STRATEJISI : card1, card2 , addr1 ve addr2 ile kombinasyonlarindan yeni degiskenler turetilebilir. \n",
    "\n",
    "\n",
    "LGBM_cat_col_list.append('addr1')\n",
    "LGBM_cat_col_list.append('addr2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist1 & dist2 --> SAYISAL DEGISKENLER ENCODING YAPMAYA GEREK YOK \n",
    "# NAN DEGER STRATEJISI :  !!! BELIRLENECEK. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P_email_domain & R_email_domain --> kategorik degiskenler. \n",
    "#  NAN DEGER STRATEJISI :  Unknown olarak belirtilebilir.  \n",
    "#  SINIF SAYISI AZALTILACAK --> Others diye belirtilecek \n",
    "\n",
    "#P_email_domain SINIF SAYISININ AZALTILMASI\n",
    "train_test.loc[train_test['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n",
    "train_test.loc[train_test['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk','yahoo.co.jp', 'yahoo.de', 'yahoo.fr','yahoo.es']), 'P_emaildomain'] = 'Yahoo'\n",
    "train_test.loc[train_test['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', 'hotmail.es','hotmail.co.uk', 'hotmail.de','outlook.es', 'live.com', 'live.fr','hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\n",
    "train_test.loc[train_test['P_emaildomain'].isin(train_test['P_emaildomain'].value_counts()[train_test['P_emaildomain'].value_counts() <= 500 ].index), 'P_emaildomain'] = \"Others\"\n",
    "train_test['P_emaildomain'].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# R_email_domain SINIF SAYISININ AZALTILMASI\n",
    "train_test.loc[train_test['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n",
    "train_test.loc[train_test['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk','yahoo.co.jp', 'yahoo.de', 'yahoo.fr','yahoo.es']), 'R_emaildomain'] = 'Yahoo'\n",
    "train_test.loc[train_test['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', 'hotmail.es','hotmail.co.uk', 'hotmail.de','outlook.es', 'live.com', 'live.fr','hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\n",
    "train_test.loc[train_test['R_emaildomain'].isin(train_test['R_emaildomain'].value_counts()[train_test['R_emaildomain'].value_counts() <= 300 ].index), 'R_emaildomain'] = \"Others\"\n",
    "train_test['R_emaildomain'].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "OHE_col_list.append('P_emaildomain')\n",
    "OHE_col_list.append('R_emaildomain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1 - C14 columns -->sadece 3 er tane nan deger var, \n",
    "# Hepsi sayisal oldugu icin direk modele sokulabilir. \n",
    "train_test[train_test['C14'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D1 - D15 columns --> Time deltas , negatif degerler var , \n",
    "# normalizasyon yapilmis chris in notbookunda \n",
    "# simdilik bu haliyle kalsin.\n",
    "\n",
    "#for i in range(1,16):\n",
    "#    if i in [1,2,3,5,9]: continue\n",
    "#    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n",
    "#    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1 - M9\n",
    "for i in range(1,10):\n",
    "    LE_col_list.append('M'+str(i))\n",
    "\n",
    "LE_col_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 - V339\n",
    "# Ayri bir kernel da pca deneyecegim simdilik boyle kalsin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id1 - id38\n",
    "# cok fzla eksik veri var ??? 800 bin den fazla\n",
    "# id12-id38 arasi kategorik 100 den fazla sinifi olanlara lgbm, 10 dan kucuk olanlara ohe digerlerine le uygulayacagim sonra degisebilir\n",
    "# tabi lgbm sadece numerik lere uygulanabiliyor. \n",
    "\n",
    "id_cols = [c for c in train_test if c[:2] == 'id']\n",
    "# train_test[id_cols].nunique()\n",
    "# train_test[id_cols].isnull().sum()\n",
    "\n",
    "#OHE_col_list.append()\n",
    "#LE_col_list.append(id_cols)\n",
    "#LGBM_cat_col_list.append()\n",
    "\n",
    "for col in id_cols:\n",
    "    LE_col_list.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeviceType --> 2 farkli deger var le uygulayacagim\n",
    "train_test.DeviceType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Info -->2799 deger var ve kategorik le uygulayacagim. \n",
    "# gruplama yapilip sinif sayisi azaltilabilir samsung, ios ... others gibi \n",
    "LE_col_list.append('DeviceInfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LE_col_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['NewDate']=train_test['NewDate'].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_test.columns:\n",
    "    if train_test[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_test[col].astype(str).values))\n",
    "        train_test[col] = le.transform(list(train_test[col].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_cat_col_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(train_test,target):\n",
    "\n",
    "    train = train_test[train_test[target].notnull()]\n",
    "    test = train_test[train_test[target].isnull()]\n",
    "\n",
    "    folds = KFold(n_splits = 10, shuffle = True, random_state = 1001)\n",
    "\n",
    "    oof_preds = np.zeros(train.shape[0])\n",
    "    sub_preds = np.zeros(test.shape[0])\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    features = [f for f in train.columns if f not in [target,'TransactionID','New_TransactionAmt_Bin','NewDate']]\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train[features], train[target])):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print('Training on fold {}'.format(n_fold + 1))\n",
    "\n",
    "        X_train, y_train = train[features].iloc[train_idx], train[target].iloc[train_idx]\n",
    "\n",
    "        X_valid, y_valid = train[features].iloc[valid_idx], train[target].iloc[valid_idx]\n",
    "       \n",
    "        clf = LGBMClassifier(num_leaves =  256,\n",
    "                             min_child_samples= 79,\n",
    "                             objective = 'binary',\n",
    "                             max_depth = 13,\n",
    "                             learning_rate= 0.03,\n",
    "                             boosting_type= \"gbdt\",\n",
    "                             subsample_freq= 3,\n",
    "                             subsample= 0.9,\n",
    "                             bagging_seed= 11,\n",
    "                             metric='auc',\n",
    "                             verbosity= -1,\n",
    "                             reg_alpha= 0.3,\n",
    "                             reg_lambda= 0.3,\n",
    "                             colsample_bytree= 0.9)\n",
    "                             #categorical_feature = LGBM_cat_col_list)\n",
    "\n",
    "        clf.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_valid, y_valid)], \n",
    "                eval_metric = 'auc', verbose = 200, early_stopping_rounds = 200)\n",
    "\n",
    "        #y_pred_valid\n",
    "        oof_preds[valid_idx] = clf.predict_proba(X_valid, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test[features], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = features\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(y_valid, oof_preds[valid_idx]))) \n",
    "\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train[target], oof_preds)) #y_pred_valid   \n",
    "\n",
    "    test[target] = sub_preds\n",
    "    test[['TransactionID', target]].to_csv(\"submission_lightgbm.csv\", index= False)\n",
    "\n",
    "    display_importances(feature_importance_df)\n",
    "    \n",
    "    return feature_importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = reduce_mem_usage(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling(train_test,'isFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODE TOGETHER\n",
    "def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df = pd.concat([df1[col],df2[col]])\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype('float32')\n",
    "        df2[nm] = df2[col].map(vc)\n",
    "        df2[nm] = df2[nm].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODE TOGETHER\n",
    "def encode_FE(df, cols):\n",
    "    for col in cols:\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df[nm] = df1[col].map(vc)\n",
    "        df[nm] = df1[nm].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
